{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5863fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Masking\n",
    "\n",
    "# 1. Load dataset\n",
    "df = pd.read_csv('/mnt/data/unaccented_data.csv')\n",
    "words = df['Word'].astype(str).tolist()\n",
    "accents = df['Accent index'].astype(int).tolist()\n",
    "\n",
    "# 2. Build character vocabulary\n",
    "chars = sorted({c for w in words for c in w})\n",
    "char2idx = {c: i+1 for i, c in enumerate(chars)}  # reserve 0 for padding\n",
    "vocab_size = len(char2idx) + 1\n",
    "\n",
    "# 3. Encode words as sequences of char indices\n",
    "encoded = [[char2idx[c] for c in w] for w in words]\n",
    "max_len = max(len(seq) for seq in encoded)\n",
    "padded = pad_sequences(encoded, maxlen=max_len, padding='post')\n",
    "\n",
    "# 4. Prepare targets: one-hot over possible positions\n",
    "targets = to_categorical(accents, num_classes=max_len)\n",
    "\n",
    "# 5. Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    padded, targets, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 6. Build model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=64, input_length=max_len, mask_zero=True),\n",
    "    Bidirectional(LSTM(64, return_sequences=False)),\n",
    "    Dense(max_len, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 7. Train\n",
    "ehistory = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# 8. Evaluate\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {loss:.4f}, Test accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d135b40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 2.0787\n",
      "Epoch 2/50, Loss: 1.4942\n",
      "Epoch 3/50, Loss: 1.2251\n",
      "Epoch 4/50, Loss: 1.0954\n",
      "Epoch 5/50, Loss: 0.9644\n",
      "Epoch 6/50, Loss: 0.8600\n",
      "Epoch 7/50, Loss: 0.7589\n",
      "Epoch 8/50, Loss: 0.6832\n",
      "Epoch 9/50, Loss: 0.5884\n",
      "Epoch 10/50, Loss: 0.5387\n",
      "Epoch 11/50, Loss: 0.4690\n",
      "Epoch 12/50, Loss: 0.4276\n",
      "Epoch 13/50, Loss: 0.3789\n",
      "Epoch 14/50, Loss: 0.3478\n",
      "Epoch 15/50, Loss: 0.3169\n",
      "Epoch 16/50, Loss: 0.2818\n",
      "Epoch 17/50, Loss: 0.2645\n",
      "Epoch 18/50, Loss: 0.2297\n",
      "Epoch 19/50, Loss: 0.2008\n",
      "Epoch 20/50, Loss: 0.1812\n",
      "Epoch 21/50, Loss: 0.1706\n",
      "Epoch 22/50, Loss: 0.1456\n",
      "Epoch 23/50, Loss: 0.1356\n",
      "Epoch 24/50, Loss: 0.1257\n",
      "Epoch 25/50, Loss: 0.1188\n",
      "Epoch 26/50, Loss: 0.0989\n",
      "Epoch 27/50, Loss: 0.0742\n",
      "Epoch 28/50, Loss: 0.0751\n",
      "Epoch 29/50, Loss: 0.0733\n",
      "Epoch 30/50, Loss: 0.0582\n",
      "Epoch 31/50, Loss: 0.0477\n",
      "Epoch 32/50, Loss: 0.0432\n",
      "Epoch 33/50, Loss: 0.0366\n",
      "Epoch 34/50, Loss: 0.0299\n",
      "Epoch 35/50, Loss: 0.0256\n",
      "Epoch 36/50, Loss: 0.0302\n",
      "Epoch 37/50, Loss: 0.0283\n",
      "Epoch 38/50, Loss: 0.0240\n",
      "Epoch 39/50, Loss: 0.0202\n",
      "Epoch 40/50, Loss: 0.0154\n",
      "Epoch 41/50, Loss: 0.0147\n",
      "Epoch 42/50, Loss: 0.0114\n",
      "Epoch 43/50, Loss: 0.0100\n",
      "Epoch 44/50, Loss: 0.0092\n",
      "Epoch 45/50, Loss: 0.0084\n",
      "Epoch 46/50, Loss: 0.0076\n",
      "Epoch 47/50, Loss: 0.0069\n",
      "Epoch 48/50, Loss: 0.0064\n",
      "Epoch 49/50, Loss: 0.0059\n",
      "Epoch 50/50, Loss: 0.0055\n",
      "Test Accuracy: 0.8184\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Load dataset\n",
    "df = pd.read_csv('unaccented_data.csv')\n",
    "words = df['Word'].astype(str).tolist()\n",
    "accents = df['Accent index'].astype(int).tolist()\n",
    "\n",
    "# 2. Build character vocabulary\n",
    "chars = sorted({c for w in words for c in w})\n",
    "char2idx = {c: i+1 for i, c in enumerate(chars)}  # reserve 0 for padding\n",
    "vocab_size = len(char2idx) + 1\n",
    "max_len = max(len(w) for w in words)\n",
    "\n",
    "# 3. Encode words as sequences of indices\n",
    "encoded_seqs = [torch.tensor([char2idx[c] for c in w], dtype=torch.long) for w in words]\n",
    "labels = torch.tensor(accents, dtype=torch.long)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "def collate_fn(batch):\n",
    "    seqs, labs = zip(*batch)\n",
    "    lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    padded_seqs = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "    return padded_seqs, lengths, torch.tensor(labs, dtype=torch.long)\n",
    "\n",
    "class AccentDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "# 4. Split data\n",
    "indices = list(range(len(encoded_seqs)))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "train_dataset = AccentDataset([encoded_seqs[i] for i in train_idx], labels[train_idx])\n",
    "test_dataset = AccentDataset([encoded_seqs[i] for i in test_idx], labels[test_idx])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# 5. Define the Bi-LSTM model\n",
    "class AccentModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, (h_n, _) = self.lstm(packed)\n",
    "        # concatenate final forward and backward hidden states\n",
    "        h_forward = h_n[-2]\n",
    "        h_backward = h_n[-1]\n",
    "        h_final = torch.cat((h_forward, h_backward), dim=1)\n",
    "        out = self.fc(h_final)\n",
    "        return out\n",
    "\n",
    "# 6. Initialize model, loss, optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AccentModel(vocab_size, embed_dim=64, hidden_dim=64, output_dim=max_len).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 7. Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for seqs, lengths, labs in train_loader:\n",
    "        seqs, lengths, labs = seqs.to(device), lengths.to(device), labs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(seqs, lengths)\n",
    "        loss = criterion(outputs, labs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# 8. Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for seqs, lengths, labs in test_loader:\n",
    "        seqs, lengths, labs = seqs.to(device), lengths.to(device), labs.to(device)\n",
    "        outputs = model(seqs, lengths)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labs).sum().item()\n",
    "        total += labs.size(0)\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "846e8da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted accent index for 'njonjast': 2\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    sample_word = \"njonjast\"\n",
    "    sample_seq = torch.tensor([char2idx.get(c, 0) for c in sample_word], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    sample_length = torch.tensor([len(sample_seq)], dtype=torch.long).to(device)\n",
    "    model.eval()\n",
    "    output = model(sample_seq, sample_length)\n",
    "    predicted_accent = output.argmax(dim=1).item()\n",
    "    print(f\"Predicted accent index for '{sample_word}': {predicted_accent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01c3514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted accent index for 'njonjast': 2\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    sample_word = \"njonjast\"\n",
    "    sample_seq = torch.tensor([char2idx.get(c, 0) for c in sample_word], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    sample_length = torch.tensor([len(sample_seq)], dtype=torch.long).to(device)\n",
    "    model.eval()\n",
    "    output = model(sample_seq, sample_length)\n",
    "    predicted_accent = output.argmax(dim=1).item()\n",
    "    print(f\"Predicted accent index for '{sample_word}': {predicted_accent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10135284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8facd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doris/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=2.1074, Val Loss=1.6697\n",
      "Epoch 2: Train Loss=1.5474, Val Loss=1.3079\n",
      "Epoch 3: Train Loss=1.3069, Val Loss=1.1576\n",
      "Epoch 4: Train Loss=1.1684, Val Loss=1.0630\n",
      "Epoch 5: Train Loss=1.0821, Val Loss=0.9966\n",
      "Epoch 6: Train Loss=1.0016, Val Loss=0.9700\n",
      "Epoch 7: Train Loss=0.9168, Val Loss=0.8995\n",
      "Epoch 8: Train Loss=0.8295, Val Loss=0.8660\n",
      "Epoch 9: Train Loss=0.7728, Val Loss=0.8116\n",
      "Epoch 10: Train Loss=0.6988, Val Loss=0.7964\n",
      "Epoch 11: Train Loss=0.6619, Val Loss=0.7673\n",
      "Epoch 12: Train Loss=0.6130, Val Loss=0.7158\n",
      "Epoch 13: Train Loss=0.5416, Val Loss=0.7204\n",
      "Epoch 14: Train Loss=0.5014, Val Loss=0.6905\n",
      "Epoch 15: Train Loss=0.4917, Val Loss=0.7142\n",
      "Epoch 16: Train Loss=0.4437, Val Loss=0.6354\n",
      "Epoch 17: Train Loss=0.4088, Val Loss=0.6291\n",
      "Epoch 18: Train Loss=0.3756, Val Loss=0.6307\n",
      "Epoch 19: Train Loss=0.3531, Val Loss=0.6479\n",
      "Epoch 20: Train Loss=0.3292, Val Loss=0.6144\n",
      "Epoch 21: Train Loss=0.2742, Val Loss=0.6492\n",
      "Epoch 22: Train Loss=0.2854, Val Loss=0.6087\n",
      "Epoch 23: Train Loss=0.2649, Val Loss=0.6561\n",
      "Epoch 24: Train Loss=0.2534, Val Loss=0.6381\n",
      "Epoch 25: Train Loss=0.2194, Val Loss=0.6641\n",
      "Epoch 26: Train Loss=0.2008, Val Loss=0.6561\n",
      "Epoch 27: Train Loss=0.1971, Val Loss=0.6164\n",
      "Early stopping triggered\n",
      "Test Accuracy: 0.8220\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. Create train/val/test splits\n",
    "indices = list(range(len(encoded_seqs)))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.1, random_state=42)  # 10% of train for validation\n",
    "\n",
    "# Collate function for padding\n",
    "def collate_fn(batch):\n",
    "    seqs, labs = zip(*batch)\n",
    "    lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    padded_seqs = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "    return padded_seqs, lengths, torch.tensor(labs, dtype=torch.long)\n",
    "\n",
    "class AccentDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "# Prepare datasets and loaders\n",
    "train_dataset = AccentDataset([encoded_seqs[i] for i in train_idx], labels[train_idx])\n",
    "val_dataset   = AccentDataset([encoded_seqs[i] for i in val_idx],   labels[val_idx])\n",
    "test_dataset  = AccentDataset([encoded_seqs[i] for i in test_idx],  labels[test_idx])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,  collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# 5. Define Bi-LSTM model with dropout\n",
    "class AccentModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, dropout_p=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True, dropout=dropout_p)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, (h_n, _) = self.lstm(packed)\n",
    "        h_forward = h_n[-2]\n",
    "        h_backward = h_n[-1]\n",
    "        h_final = torch.cat((h_forward, h_backward), dim=1)\n",
    "        out = self.dropout(h_final)\n",
    "        return self.fc(out)\n",
    "\n",
    "# 6. Initialize model, loss, optimizer (with weight decay)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AccentModel(vocab_size, embed_dim=64, hidden_dim=64, output_dim=max_len, dropout_p=0.3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "# 7. Training loop with early stopping\n",
    "num_epochs = 30\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for seqs, lengths, labs in train_loader:\n",
    "        seqs, lengths, labs = seqs.to(device), lengths.to(device), labs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(seqs, lengths)\n",
    "        loss = criterion(outputs, labs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    avg_train = train_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for seqs, lengths, labs in val_loader:\n",
    "            seqs, lengths, labs = seqs.to(device), lengths.to(device), labs.to(device)\n",
    "            outputs = model(seqs, lengths)\n",
    "            val_loss += criterion(outputs, labs).item()\n",
    "    avg_val = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={avg_train:.4f}, Val Loss={avg_val:.4f}\")\n",
    "\n",
    "    # Check early stopping\n",
    "    if avg_val < best_val_loss:\n",
    "        best_val_loss = avg_val\n",
    "        epochs_no_improve = 0\n",
    "        best_state = model.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "if best_state:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "# 8. Final evaluation on test set\n",
    "model.eval()\n",
    "correct = total = 0\n",
    "with torch.no_grad():\n",
    "    for seqs, lengths, labs in test_loader:\n",
    "        seqs, lengths, labs = seqs.to(device), lengths.to(device), labs.to(device)\n",
    "        preds = model(seqs, lengths).argmax(1)\n",
    "        correct += (preds == labs).sum().item()\n",
    "        total += labs.size(0)\n",
    "print(f\"Test Accuracy: {correct/total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d8c6cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21936fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doris/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=2.1457, Val Loss=1.6937\n",
      "Epoch 2: Train Loss=1.5393, Val Loss=1.2807\n",
      "Epoch 3: Train Loss=1.2609, Val Loss=1.1175\n",
      "Epoch 4: Train Loss=1.1438, Val Loss=1.0677\n",
      "Epoch 5: Train Loss=1.0571, Val Loss=0.9915\n",
      "Epoch 6: Train Loss=0.9705, Val Loss=0.9583\n",
      "Epoch 7: Train Loss=0.8891, Val Loss=0.9051\n",
      "Epoch 8: Train Loss=0.8128, Val Loss=0.8390\n",
      "Epoch 9: Train Loss=0.7283, Val Loss=0.7952\n",
      "Epoch 10: Train Loss=0.6734, Val Loss=0.7530\n",
      "Epoch 11: Train Loss=0.6063, Val Loss=0.7145\n",
      "Epoch 12: Train Loss=0.5532, Val Loss=0.6821\n",
      "Epoch 13: Train Loss=0.5226, Val Loss=0.6994\n",
      "Epoch 14: Train Loss=0.4698, Val Loss=0.6456\n",
      "Epoch 15: Train Loss=0.4321, Val Loss=0.6892\n",
      "Epoch 16: Train Loss=0.4127, Val Loss=0.6221\n",
      "Epoch 17: Train Loss=0.3701, Val Loss=0.6244\n",
      "Epoch 18: Train Loss=0.3651, Val Loss=0.6309\n",
      "Epoch 19: Train Loss=0.3188, Val Loss=0.6048\n",
      "Epoch 20: Train Loss=0.3077, Val Loss=0.6287\n",
      "Epoch 21: Train Loss=0.2894, Val Loss=0.6280\n",
      "Epoch 22: Train Loss=0.2546, Val Loss=0.6028\n",
      "Epoch 23: Train Loss=0.2516, Val Loss=0.6289\n",
      "Epoch 24: Train Loss=0.2256, Val Loss=0.6128\n",
      "Epoch 25: Train Loss=0.2006, Val Loss=0.6299\n",
      "Epoch 26: Train Loss=0.1962, Val Loss=0.6190\n",
      "Epoch 27: Train Loss=0.1821, Val Loss=0.6278\n",
      "Early stopping triggered\n",
      "Test Accuracy: 0.8172\n"
     ]
    }
   ],
   "source": [
    "# 1. Load dataset\n",
    "df = pd.read_csv('unaccented_data.csv')\n",
    "words = df['Word'].astype(str).tolist()\n",
    "raw_accents = df['Accent index'].astype(int).tolist()  # 0 = no accent, 1..n = accent position (1-based)\n",
    "\n",
    "# 2. Build character vocabulary\n",
    "chars = sorted({c for w in words for c in w})\n",
    "char2idx = {c: i+1 for i, c in enumerate(chars)}  # reserve 0 for padding\n",
    "vocab_size = len(char2idx) + 1\n",
    "max_len = max(len(w) for w in words)\n",
    "\n",
    "# 3. Encode words as sequences of indices\n",
    "encoded_seqs = [torch.tensor([char2idx[c] for c in w], dtype=torch.long) for w in words]\n",
    "\n",
    "# 4. Prepare labels: keep 0 as 'no accent', positions are already 1-based\n",
    "labels = torch.tensor(raw_accents, dtype=torch.long)\n",
    "\n",
    "# 5. Create train/val/test splits\n",
    "indices = list(range(len(encoded_seqs)))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.1, random_state=42)\n",
    "\n",
    "# Collate function\n",
    "def collate_fn(batch):\n",
    "    seqs, labs = zip(*batch)\n",
    "    lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    padded_seqs = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "    return padded_seqs, lengths, torch.tensor(labs, dtype=torch.long)\n",
    "\n",
    "class AccentDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "# DataLoaders\n",
    "train_ds = AccentDataset([encoded_seqs[i] for i in train_idx], labels[train_idx])\n",
    "val_ds   = AccentDataset([encoded_seqs[i] for i in val_idx],   labels[val_idx])\n",
    "test_ds  = AccentDataset([encoded_seqs[i] for i in test_idx],  labels[test_idx])\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# 6. Define Bi-LSTM model\n",
    "class AccentModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, dropout_p=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True, dropout=dropout_p)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, _) = self.lstm(packed)\n",
    "        h_final = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        out = self.dropout(h_final)\n",
    "        return self.fc(out)\n",
    "\n",
    "# 7. Initialize model, loss, optimizer\n",
    "#    output_dim = max_len + 1 to include class 0\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AccentModel(vocab_size, embed_dim=64, hidden_dim=64, output_dim=max_len+1, dropout_p=0.3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "# 8. Training with early stopping\n",
    "num_epochs = 50\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for seqs, lengths, labs in train_loader:\n",
    "        seqs, lengths, labs = seqs.to(device), lengths.to(device), labs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(seqs, lengths)\n",
    "        loss = criterion(outputs, labs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    avg_train = train_loss / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for seqs, lengths, labs in val_loader:\n",
    "            seqs, lengths, labs = seqs.to(device), lengths.to(device), labs.to(device)\n",
    "            outputs = model(seqs, lengths)\n",
    "            val_loss += criterion(outputs, labs).item()\n",
    "    avg_val = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={avg_train:.4f}, Val Loss={avg_val:.4f}\")\n",
    "    if avg_val < best_val_loss:\n",
    "        best_val_loss = avg_val\n",
    "        epochs_no_improve = 0\n",
    "        best_state = model.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "if best_state:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "# 9. Test evaluation\n",
    "model.eval()\n",
    "correct = total = 0\n",
    "with torch.no_grad():\n",
    "    for seqs, lengths, labs in test_loader:\n",
    "        seqs, lengths, labs = seqs.to(device), lengths.to(device), labs.to(device)\n",
    "        preds = model(seqs, lengths).argmax(1)\n",
    "        correct += (preds == labs).sum().item()\n",
    "        total += labs.size(0)\n",
    "print(f\"Test Accuracy: {correct/total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86b17810",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db751835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# 10. Prediction function for user input\n",
    "def predict_accent(word: str) -> int:\n",
    "    \"\"\"\n",
    "    Predict accent position for a single Croatian word.\n",
    "    Returns 0 if no accent, or 1-based index of accented character.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # Encode and pad\n",
    "    seq = torch.tensor([char2idx.get(c, 0) for c in word], dtype=torch.long)\n",
    "    length = torch.tensor([len(seq)], dtype=torch.long)\n",
    "    padded = pad_sequence([seq], batch_first=True, padding_value=0)\n",
    "    padded = padded.to(device)\n",
    "    length = length.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(padded, length)\n",
    "        pred = logits.argmax(dim=1).item()\n",
    "    return pred\n",
    "\n",
    "# Example usage:\n",
    "print(predict_accent(\"mnogokut\"))  # prints predicted index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32b6e013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accent at pos 2, character 'i'\n"
     ]
    }
   ],
   "source": [
    "def predict_accent(word):\n",
    "    \"\"\"\n",
    "    Predict accent position for a single Croatian word.\n",
    "    Returns a tuple (position, character), where position=0 means no accent and character=None.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # Encode and pad\n",
    "    seq = torch.tensor([char2idx.get(c, 0) for c in word], dtype=torch.long)\n",
    "    length = torch.tensor([len(seq)], dtype=torch.long)\n",
    "    padded = pad_sequence([seq], batch_first=True, padding_value=0).to(device)\n",
    "    length = length.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(padded, length)\n",
    "        pred = logits.argmax(dim=1).item()\n",
    "    if pred == 0:\n",
    "        return 0, None\n",
    "    else:\n",
    "        # pred is 1-based index into word\n",
    "        accented_char = word[pred-1] if pred-1 < len(word) else None\n",
    "        return pred, accented_char\n",
    "\n",
    "# Example usage:\n",
    "pos, char = predict_accent(\"riječ\")\n",
    "print(f\"Accent at pos {pos}, character '{char}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "367c7051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accent at pos 4, character 'i'\n"
     ]
    }
   ],
   "source": [
    "pos, char = predict_accent(\"jačina\")\n",
    "print(f\"Accent at pos {pos}, character '{char}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
